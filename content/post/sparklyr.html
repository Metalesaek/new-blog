---
title: "Introduction to sparklyr"
author: "Abdelkader Metales"
date: '2019-01-23'
tags:
- sparklyr
- Big data
- Machine learning
categories: R
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The programming language R has very powerful tools and functions to do almost every thing we want to do, such as wrangling , visualizing, modeling…etc. However, R such as all the classical languages, requires the whole data to be completely loaded into its memory before doing anything, and this is a big disadvantage when we deal with large data set using less powerful machine, so that any even small data manipulation is time consuming, and may be in some cases the data size can exceed the memory size and R fails even to load the data.</p>
<p>However, there are two widely used engines for this type of data <strong>hadoop</strong> and <strong>spark</strong> which both use a distributed system to partition the data into different storage locations and distribute any computation processes among different machines (computing clusters), or among different CPU’s inside a single machine.</p>
<p>Spark is more recent and recognized to be more faster than hadoop (2010). <strong>scala</strong> is its native language, but it can also support <strong>SQL</strong> and <strong>java</strong>. Obviously, if you do not know neither spark nor hadoop it would be obvious to choose spark . However, if you are R user and you do not want to spent time to learn the spark languages (scala, or sql) good news for you is that <strong>sparklyr</strong> package (or sparkR) is R interface for spark from which you can use the most of the R codes and other functions from some packages such as dplyr …etc.</p>
<p>In this paper we will go step by step to learn how to use sparklyr by making use of some examples .</p>
</div>
<div id="installing-sparklyr" class="section level2">
<h2>Installing sparklyr</h2>
<p>Such as any R package we call the function <strong>install.packages</strong>
to install sparklyr, but before that make sure you have <strong>java</strong> installed in your system since the programming language <strong>scala</strong> is run by the java virtual machine.</p>
<pre class="r"><code>#install.packages(&quot;sparklyr&quot;)</code></pre>
</div>
<div id="installing-spark" class="section level2">
<h2>Installing spark</h2>
<p>We have deliberately installed sparklyr before spark to provide us with the function <strong>spark_install()</strong> that downloads, installs, and configures the latest version of spark at once.</p>
<pre class="r"><code>#spark_install()</code></pre>
</div>
<div id="connecting-to-spark" class="section level2">
<h2>Connecting to spark</h2>
<p>Usually, spark is designed to create a clusters using multiple machines either physical machines or virtual machines (in the cloud). However, it can also create a local cluster in your single machine by making use of the CPU’s, if exist in this machine, to speed up the data processing.</p>
<p>Wherever the clusters are created (local or in cloud), the data processing functions work in the same way, and the only difference is how to create and interact with these clusters. Since this is the case, then we can get started in our local cluster to learn the most basic things of data science such as importing, analyzing, visualizing data, and perform machine learning models using spark via sparklyr.</p>
<p>To connect to spark in the local mode we use the function <strong>spark_connect</strong> as follows.</p>
<pre class="r"><code>library(sparklyr)
library(tidyverse)
sc&lt;-spark_connect(master = &quot;local&quot;)</code></pre>
</div>
<div id="importing-data" class="section level2">
<h2>Importing data</h2>
<p>If the data is build-in R we load it to the spark memory using the function <strong>copy_to</strong>.</p>
<pre class="r"><code>mydata&lt;-copy_to(sc,airquality)</code></pre>
<p>Then R can get access to this data by the help of sparklyr, for example we can use the dplyr function <strong>glimpse</strong>.</p>
<pre class="r"><code>glimpse(mydata)</code></pre>
<pre><code>## Observations: ??
## Variables: 6
## Database: spark_connection
## $ Ozone   &lt;int&gt; 41, 36, 12, 18, NA, 28, 23, 19, 8, NA, 7, 16, 11, 14, 18, 1...
## $ Solar_R &lt;int&gt; 190, 118, 149, 313, NA, NA, 299, 99, 19, 194, NA, 256, 290,...
## $ Wind    &lt;dbl&gt; 7.4, 8.0, 12.6, 11.5, 14.3, 14.9, 8.6, 13.8, 20.1, 8.6, 6.9...
## $ Temp    &lt;int&gt; 67, 72, 74, 62, 56, 66, 65, 59, 61, 69, 74, 69, 66, 68, 58,...
## $ Month   &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,...
## $ Day     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, ...</code></pre>
<p>And if the data is stored anywhere outside R with any different format, then sparklyr provides some functions to import these data. For example to load csv file we use the function <strong>spark_read_csv</strong>, and for json we use <strong>spark_read_json</strong>. To get the list of all the sparklyr functions and their usages click <a href="https://cran.r-project.org/web/packages/sparklyr/sparklyr.pdf">here</a>.</p>
<p>For illustration we will call tha data <strong>creditcards</strong> stored in my machine as follows</p>
<pre class="r"><code>card&lt;-spark_read_csv(sc,&quot;creditcard.csv&quot;)
sdf_dim(card)</code></pre>
<pre><code>## [1] 284807     31</code></pre>
<p>As you see using the same connection <strong>sc</strong> we load two data <strong>mydata</strong> and <strong>card</strong></p>
<p>if we want to show what is going on in spark we call the function <strong>spark_web()</strong> that lead us to the spark website</p>
<pre class="r"><code>#spark_web(sc)</code></pre>
</div>
<div id="manipulating-data" class="section level2">
<h2>Manipulating data</h2>
<p>With the help of sparklyr, we can access very easily to the data into spark memory by using the delyr functions. Let’s apply some manipulations on the data <strong>card</strong> such as for example filtering the data using the variable <strong>Time</strong> , then we compute the mean of <strong>Amount</strong> for each class label in the variable <strong>Class</strong>.</p>
<pre class="r"><code>card %&gt;%
  filter(Time &lt;= mean(Time,na.rm = TRUE))%&gt;%
      group_by(Class)%&gt;%
  summarise(Class_avg=mean(Amount,na.rm=TRUE))</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 2]
##   Class Class_avg
##   &lt;int&gt;     &lt;dbl&gt;
## 1     0      89.0
## 2     1     117.</code></pre>
<p>As you can see now the output is a very small table which can moved from spark memory into R memory for further analysis by making use of the function <strong>collect</strong>. In other words, if you feel with ease in R then each spark output that is small enough to be processed with R add this function at the end of your script before running it to bring this output into R. For example we cannot use the function <strong>plot</strong> to plot the above table, that is why we should fist pull this output into R then apply the function <strong>plot</strong> as follows</p>
<pre class="r"><code>card %&gt;%
  filter(Time &lt;= mean(Time,na.rm = TRUE))%&gt;%
      group_by(Class)%&gt;%
  summarise(Class_avg=mean(Amount,na.rm=TRUE))%&gt;%
  collect()%&gt;%
  plot(col=&quot;red&quot;,pch=19,main = &quot;Class average vs Class&quot;)</code></pre>
<p><img src="/post/sparklyr_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>However , we can plot the sparklyr outputs without having to remove them to R memory by using the <strong>dbplot</strong> functions, since most of the functions of this package are supported by sparklyr. Let’s for example plot the mean of Amount by Class for cards transaction that have time less than the mean.</p>
<pre class="r"><code>library(dbplot)
card %&gt;%
  filter(Time &lt;= mean(Time,na.rm = TRUE))%&gt;%
        dbplot_bar(Class,mean(Amount))</code></pre>
<pre><code>## Warning: Missing values are always removed in SQL.
## Use `mean(x, na.rm = TRUE)` to silence this warning
## This warning is displayed only once per session.</code></pre>
<p><img src="/post/sparklyr_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>As we see the Amount mean of fraudulent cards is higher than that of regular cards.</p>
</div>
<div id="disconnecting" class="section level2">
<h2>Disconnecting</h2>
<p>each time you finish your work think to disconnect from spark to save your resources as follows.</p>
<pre class="r"><code>#spark_disconnect(sc)</code></pre>
</div>
<div id="saving-data" class="section level2">
<h2>saving data</h2>
<p>Sparklyr provides functions to save files directly from spark memory into our directory. For example, to save data in csv file we use spark function <strong>spark_write_csv</strong> (we can save in other type of formats such as <strong>spark_write_parquet</strong>,…etc) as follows</p>
<pre class="r"><code>#spark_write_csv(card,&quot;card.csv&quot;)</code></pre>
</div>
<div id="example-of-modeling-in-spark" class="section level2">
<h2>Example of modeling in spark</h2>
<p>For machine learning models spark has its own library <strong>MLlib</strong> that has almost every thing we need so that we do not need the library <strong>caret</strong>.</p>
<p>To illustrate how do we perform a machine learning model, we train a logistic regression model to predict the fraudulent cards form the data <strong>card</strong>.</p>
<p>first let’s split the data between training set and testing set as follows, and to do this we use the function <strong>sdf_random_split</strong> as follows</p>
<pre class="r"><code>partitions&lt;-card%&gt;%
  sdf_random_split(training=0.8,test=0.2,seed = 123)
train&lt;-partitions$training
test&lt;-partitions$test</code></pre>
<p>Now we will use the set <strong>train</strong> to train our model, and for the model performance we make use of the set <strong>test</strong>.</p>
<pre class="r"><code>model_in_spark&lt;-train %&gt;%
  ml_logistic_regression(Class~.)</code></pre>
<p>we can get the summary of this model by typing its name</p>
<pre class="r"><code>model_in_spark</code></pre>
<pre><code>## Formula: Class ~ .
## 
## Coefficients:
##   (Intercept)          Time            V1            V2            V3 
## -8.305599e+00 -4.074154e-06  1.065118e-01  1.473891e-02 -8.426563e-03 
##            V4            V5            V6            V7            V8 
##  6.996793e-01  1.380980e-01 -1.217416e-01 -1.205822e-01 -1.700146e-01 
##            V9           V10           V11           V12           V13 
## -2.734966e-01 -8.277600e-01 -4.476393e-02  7.416858e-02 -2.828732e-01 
##           V14           V15           V16           V17           V18 
## -5.317753e-01 -1.221061e-01 -2.476344e-01 -1.591295e-03  3.403402e-02 
##           V19           V20           V21           V22           V23 
##  9.213132e-02 -4.914719e-01  3.863870e-01  6.407714e-01 -1.096256e-01 
##           V24           V25           V26           V27           V28 
##  1.366914e-01 -5.108841e-02  9.977837e-02 -8.384655e-01 -3.072630e-01 
##        Amount 
##  1.039041e-03</code></pre>
<p>Fortunately, sparklyr also supports the functions of <strong>broom</strong> package so that We can get nicer table using the function <strong>tidy</strong>.</p>
<pre class="r"><code>library(broom)
tidy(model_in_spark)</code></pre>
<pre><code>## # A tibble: 31 x 2
##    features    coefficients
##    &lt;chr&gt;              &lt;dbl&gt;
##  1 (Intercept)  -8.31      
##  2 Time         -0.00000407
##  3 V1            0.107     
##  4 V2            0.0147    
##  5 V3           -0.00843   
##  6 V4            0.700     
##  7 V5            0.138     
##  8 V6           -0.122     
##  9 V7           -0.121     
## 10 V8           -0.170     
## # ... with 21 more rows</code></pre>
<p>To evaluate the model performance we use the function <strong>ml_evaluate</strong> as follows</p>
<pre class="r"><code>model_summary&lt;-ml_evaluate(model_in_spark,train)
model_summary</code></pre>
<pre><code>## BinaryLogisticRegressionSummaryImpl 
##  Access the following via `$` or `ml_summary()`. 
##  - features_col() 
##  - label_col() 
##  - predictions() 
##  - probability_col() 
##  - area_under_roc() 
##  - f_measure_by_threshold() 
##  - pr() 
##  - precision_by_threshold() 
##  - recall_by_threshold() 
##  - roc() 
##  - prediction_col() 
##  - accuracy() 
##  - f_measure_by_label() 
##  - false_positive_rate_by_label() 
##  - labels() 
##  - precision_by_label() 
##  - recall_by_label() 
##  - true_positive_rate_by_label() 
##  - weighted_f_measure() 
##  - weighted_false_positive_rate() 
##  - weighted_precision() 
##  - weighted_recall() 
##  - weighted_true_positive_rate()</code></pre>
<p>To extract the metric that we want we use <strong>$</strong>. we can extract for example <strong>the accuracy rate</strong>, the <strong>AUC</strong> or the <strong>roc</strong></p>
<pre class="r"><code>model_summary$area_under_roc()</code></pre>
<pre><code>## [1] 0.9765371</code></pre>
<pre class="r"><code>model_summary$accuracy()</code></pre>
<pre><code>## [1] 0.999149</code></pre>
<pre class="r"><code>model_summary$roc()</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 2]
##        FPR   TPR
##      &lt;dbl&gt; &lt;dbl&gt;
##  1 0       0    
##  2 0.00849 0.876
##  3 0.0185  0.898
##  4 0.0285  0.908
##  5 0.0386  0.917
##  6 0.0487  0.922
##  7 0.0587  0.922
##  8 0.0688  0.925
##  9 0.0788  0.929
## 10 0.0888  0.934
## # ... with more rows</code></pre>
<p>we can retrieve this table into R to plot it with ggplot by using the function <strong>collect</strong></p>
<pre class="r"><code>model_summary$roc()%&gt;%
collect()%&gt;%
ggplot(aes(FPR,TPR ))+
  geom_line(col=&quot;blue&quot;)+
  geom_abline(intercept = 0,slope = 1,col=&quot;red&quot;)+
  ggtitle(&quot;the roc of model_in_spark &quot;)</code></pre>
<p><img src="/post/sparklyr_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>High accuracy rate for the training set can be only the result of overfitting problem. the accuracy rate using the testing set is the more reliable one.</p>
<pre class="r"><code>pred&lt;-ml_evaluate(model_in_spark,test)
pred$accuracy()</code></pre>
<pre><code>## [1] 0.9994722</code></pre>
<pre class="r"><code>pred$area_under_roc()</code></pre>
<pre><code>## [1] 0.9693085</code></pre>
<p>Finally, to get the prediction we use the function <strong>ml_predict</strong></p>
<pre class="r"><code>pred&lt;-ml_predict(model_in_spark,test)%&gt;%
select(.,Class,prediction,probability_0,probability_1)
pred  </code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 4]
##    Class prediction probability_0 probability_1
##    &lt;int&gt;      &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;
##  1     0          0         1.00       0.000221
##  2     0          0         1.00       0.000441
##  3     0          0         1.00       0.000184
##  4     0          0         1.00       0.000490
##  5     0          0         1.00       0.000199
##  6     0          0         0.999      0.000708
##  7     0          0         1.00       0.000231
##  8     0          0         0.999      0.000640
##  9     0          0         1.00       0.000265
## 10     0          0         0.999      0.000720
## # ... with more rows</code></pre>
<p>Here we can also use the function <strong>collect</strong> to plot the results</p>
<pre class="r"><code>pred%&gt;%
  collect()%&gt;%
  ggplot(aes(Class,prediction ))+
  geom_point(size=0.1)+
  geom_jitter()+
  ggtitle(&quot;Actual vs predicted&quot;)</code></pre>
<p><img src="/post/sparklyr_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
</div>
<div id="streaming" class="section level2">
<h2>Streaming</h2>
<p>Among the most powrful properties of spark is that can handle streaming data very easily. to show that let’s use a simple example by creating a folder to contain the input for some data transformations and then we save the output in another folder so that each time we add files to the first folder the above transformations will be excuted automotically and the output will be saved in the last folder.</p>
<pre class="r"><code>#dir.create(&quot;raw_data&quot;)</code></pre>
<p>once the file is created we split the data <strong>card</strong> into tow parts the first part will be exported now to the folder <strong>raw_data</strong>, and then we apply some operations using spark functions <strong>stream_read_csv</strong> and <strong>spark_wrirte_csv</strong> as follows .</p>
<pre class="r"><code>#card1&lt;-card%&gt;%
  #filter(Time&lt;=mean(Time,na.rm = TRUE))
#write.csv(card1,&quot;raw_data/card1.csv&quot;)</code></pre>
<pre class="r"><code>#stream &lt;- stream_read_csv(sc,&quot;raw_data/&quot;)%&gt;%
 # select(Class,Amount) %&gt;%
#  stream_write_csv(&quot;result/&quot;)</code></pre>
<p>If we add the second part in the file raw_data the streaming process luch to execute the above operation.</p>
<pre class="r"><code>#card2&lt;-card%&gt;%
 # filter(Time&gt;mean(Time,na.rm = TRUE))
#write.csv(card,&quot;raw_data/card2.csv&quot;)</code></pre>
<pre class="r"><code>#dir(&quot;result&quot;,pattern = &quot;.csv&quot;)</code></pre>
<p>we stop the stream</p>
<pre class="r"><code>#stream_stop(stream)</code></pre>
<pre class="r"><code>sdf_describe(card)</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 32]
##   summary Time  V1    V2    V3    V4    V5    V6    V7    V8    V9    V10  
##   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;
## 1 count   2848~ 2848~ 2848~ 2848~ 2848~ 2848~ 2848~ 2848~ 2848~ 2848~ 2848~
## 2 mean    9481~ 1.75~ -8.2~ -9.6~ 8.32~ 1.64~ 4.24~ -3.0~ 8.81~ -1.1~ 7.09~
## 3 stddev  4748~ 1.95~ 1.65~ 1.51~ 1.41~ 1.38~ 1.33~ 1.23~ 1.19~ 1.09~ 1.08~
## 4 min     0     -56.~ -72.~ -48.~ -5.6~ -113~ -26.~ -43.~ -73.~ -13.~ -24.~
## 5 max     1727~ 2.45~ 22.0~ 9.38~ 16.8~ 34.8~ 73.3~ 120.~ 20.0~ 15.5~ 23.7~
## # ... with 20 more variables: V11 &lt;chr&gt;, V12 &lt;chr&gt;, V13 &lt;chr&gt;, V14 &lt;chr&gt;,
## #   V15 &lt;chr&gt;, V16 &lt;chr&gt;, V17 &lt;chr&gt;, V18 &lt;chr&gt;, V19 &lt;chr&gt;, V20 &lt;chr&gt;,
## #   V21 &lt;chr&gt;, V22 &lt;chr&gt;, V23 &lt;chr&gt;, V24 &lt;chr&gt;, V25 &lt;chr&gt;, V26 &lt;chr&gt;,
## #   V27 &lt;chr&gt;, V28 &lt;chr&gt;, Amount &lt;chr&gt;, Class &lt;chr&gt;</code></pre>
<p>Now we disconnect</p>
<pre class="r"><code>spark_disconnect(sc)</code></pre>
</div>
