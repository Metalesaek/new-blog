---
title: "Predicting images using Convolutional neural network"
author: "Abdelkader Metales"
date: "2020-12-03"
output:
  blogdown::html_page:
    toc: true
    toc_depth: 1
    fig_width: 6
  keep_md: true
menu: 
  cnnmodel: 
    parent: cnnmodel Topic
    weight: 1
tags:
- CNN
categories: R

---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In this paper we will make use of the convolutional neural network, the most widely deep learning method used for image classification, object detection,..etc<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<p>In this paper we are going be learning how to build and train <strong>convolutional neural network</strong> model using small sample of images collected from google search. The data includes 30 images, each of which is either one of three types of animals: <strong>cat</strong>, <strong>dog</strong>, or <strong>lion</strong>, and each one has equally number of images, that is 10.</p>
</div>
<div id="data-preparation" class="section level2">
<h2>Data preparation</h2>
<p>First, we call the packages needed along this paper and load the data into two different objects, one called <strong>train</strong>, will contain 7 instances of each animal type used for training the model, and another one, called <strong>test</strong>, will contain the remaining instances for the evaluation of the model performance.</p>
<pre class="r"><code>library(EBImage)
library(keras)
library(foreach)

mytrain &lt;- c(paste0(&quot;images/cat&quot;,1:7,&quot;.jpg&quot;),paste0(&quot;images/dog&quot;,1:7,&quot;.jpg&quot;),
        paste0(&quot;images/lion&quot;,1:7,&quot;.jpg&quot;))

mytest &lt;- c(paste0(&quot;images/cat&quot;,8:10,&quot;.jpg&quot;),paste0(&quot;images/dog&quot;,8:10,&quot;.jpg&quot;),
        paste0(&quot;images/lion&quot;,8:10,&quot;.jpg&quot;))

train &lt;- lapply(mytrain, readImage)
test &lt;- lapply(mytest, readImage)</code></pre>
<p>Now let us first figure out what information each image contains .</p>
<pre class="r"><code>train[[1]]</code></pre>
<pre><code>## Image 
##   colorMode    : Color 
##   storage.mode : double 
##   dim          : 275 183 3 
##   frames.total : 3 
##   frames.render: 1 
## 
## imageData(object)[1:5,1:6,1]
##           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]
## [1,] 0.2039216 0.2039216 0.2039216 0.2078431 0.2078431 0.2000000
## [2,] 0.2039216 0.2039216 0.2078431 0.2078431 0.2078431 0.2039216
## [3,] 0.2078431 0.2078431 0.2078431 0.2117647 0.2117647 0.2078431
## [4,] 0.2117647 0.2117647 0.2156863 0.2156863 0.2196078 0.2117647
## [5,] 0.2156863 0.2156863 0.2196078 0.2196078 0.2235294 0.2156863</code></pre>
<p>As we see this image is color image with 275 pxl hight, 183 pxl width and 3 chanels (RGB) since it is a color image.</p>
<p>we can visualize an image as follows:</p>
<pre class="r"><code>plot(test[[4]])</code></pre>
<p><img src="/courses/cnnmodel/imageCNN_files/figure-html/unnamed-chunk-3-1.png" width="576" /></p>
<p>If instead we want to visualize all the image as one block we can make use of <strong>foreach</strong> package to apply a for loop as follows.</p>
<pre class="r"><code>par(mfrow=c(7,3))
foreach(i=1:21) %do% {plot(train[[i]])}</code></pre>
<p><img src="/courses/cnnmodel/imageCNN_files/figure-html/unnamed-chunk-4-1.png" width="576" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
<p>After taking a brief glance at our data, we found that the image sizes are different from eaech other which is not what our image classification model expects. That is why, the following script will resize all the images to have the same size <strong>150x150x3</strong>.</p>
<pre class="r"><code>foreach(i=1:21) %do% {train[[i]] &lt;- resize(train[[i]],150,150)}
foreach(i=1:9) %do% {test[[i]] &lt;- resize(test[[i]],150,150)}</code></pre>
<p>To check the result we use the following:</p>
<pre class="r"><code>str(test)</code></pre>
<pre><code>## List of 9
##  $ :Formal class &#39;Image&#39; [package &quot;EBImage&quot;] with 2 slots
##   .. ..@ .Data    : num [1:150, 1:150, 1:3] 0.761 0.78 0.773 0.755 0.768 ...
##   .. ..@ colormode: int 2
##  $ :Formal class &#39;Image&#39; [package &quot;EBImage&quot;] with 2 slots
##   .. ..@ .Data    : num [1:150, 1:150, 1:3] 0.462 0.48 0.512 0.544 0.54 ...
##   .. ..@ colormode: int 2
##  $ :Formal class &#39;Image&#39; [package &quot;EBImage&quot;] with 2 slots
##   .. ..@ .Data    : num [1:150, 1:150, 1:3] 0.986 0.992 0.951 0.945 0.929 ...
##   .. ..@ colormode: int 2
##  $ :Formal class &#39;Image&#39; [package &quot;EBImage&quot;] with 2 slots
##   .. ..@ .Data    : num [1:150, 1:150, 1:3] 0.81 0.751 0.787 0.825 0.508 ...
##   .. ..@ colormode: int 2
##  $ :Formal class &#39;Image&#39; [package &quot;EBImage&quot;] with 2 slots
##   .. ..@ .Data    : num [1:150, 1:150, 1:3] 0.361 0.502 0.49 0.627 0.524 ...
##   .. ..@ colormode: int 2
##  $ :Formal class &#39;Image&#39; [package &quot;EBImage&quot;] with 2 slots
##   .. ..@ .Data    : num [1:150, 1:150, 1:3] 0.375 0.365 0.375 0.397 0.393 ...
##   .. ..@ colormode: int 2
##  $ :Formal class &#39;Image&#39; [package &quot;EBImage&quot;] with 2 slots
##   .. ..@ .Data    : num [1:150, 1:150, 1:3] 0.651 0.57 0.614 0.636 0.63 ...
##   .. ..@ colormode: int 2
##  $ :Formal class &#39;Image&#39; [package &quot;EBImage&quot;] with 2 slots
##   .. ..@ .Data    : num [1:150, 1:150, 1:3] 0.268 0.201 0.198 0.213 0.182 ...
##   .. ..@ colormode: int 2
##  $ :Formal class &#39;Image&#39; [package &quot;EBImage&quot;] with 2 slots
##   .. ..@ .Data    : num [1:150, 1:150, 1:3] 0 0 0 0 0 ...
##   .. ..@ colormode: int 2</code></pre>
<p>As we see all the images now have the same size as an array of 3 dimension. The next step now is to combine all the images in one block.</p>
<pre class="r"><code>trainall &lt;- combine(train)
testall &lt;- combine(test) </code></pre>
<p>We can display the output block usine the following:</p>
<pre class="r"><code>display(tile(trainall,7))</code></pre>
<p><img src="/courses/cnnmodel/imageCNN_files/figure-html/unnamed-chunk-8-1.png" width="576" /></p>
<p>Now the images are nicely combined in one block with four dimension: number of instances (images), height, width, and number of channels, and this is the input that will be used in our model. However, to correctly read the input our model expects that the first dimension is the number of instances, the second is height , the third is width, and the fourth is the number of channels.
Let us check whether the input has the correct order or not.</p>
<pre class="r"><code>str(trainall)</code></pre>
<pre><code>## Formal class &#39;Image&#39; [package &quot;EBImage&quot;] with 2 slots
##   ..@ .Data    : num [1:150, 1:150, 1:3, 1:21] 0.204 0.209 0.216 0.223 0.233 ...
##   ..@ colormode: int 2</code></pre>
<p>This order is not correct since the number of instances is in the last position, so we reorder the positions as follows:</p>
<pre class="r"><code>trainall &lt;- aperm(trainall, c(4,1,2,3))
testall &lt;- aperm(testall, c(4,1,2,3))</code></pre>
<p>The Last thing that remains to be done, before customizing the architecture of our model, is to create a variable to hold the image’s labels, then convert it to a dummy variable.</p>
<pre class="r"><code>trainlabels &lt;- rep(0:2, each=7)
testlabels &lt;- rep(0:2, each=3)
trainy &lt;- to_categorical(trainlabels)
testy &lt;- to_categorical(testlabels)</code></pre>
</div>
<div id="training-the-model" class="section level2">
<h2>Training the model:</h2>
<p>The architecture of our model will contain the following layers:</p>
<ol style="list-style-type: decimal">
<li>Convolution layer that makes use of 32 filters with size 3x3 (since the input has 150x150x3 consequently the third dimension of the filter size will be 3 that is 3x3x3), and with <strong>Relu</strong> as activation function.</li>
<li>maxPooling layer of 3x3 with strides=2.</li>
<li>Convolution layer that makes use of 64 filters with size 5x5 , and with <strong>Relu</strong> function.</li>
<li>maxPooling layer of 2x2 with strides=2.</li>
<li>Convolution layer that makes use of 128 filters with size 3x3 , and with <strong>Relu</strong> function.</li>
<li>maxPooling layer of 2x2 with strides=2.</li>
<li>Flatten layer to collapse all the output elements into one giant vector to be able to connect to the traditional neural network with fully connected layers.</li>
<li>dense layers composed of 256 nodes and with <strong>leaky_relu</strong> function. The slope for thee negative part will be <strong>0.1</strong>.</li>
<li>Dropout layer with rate of 40%, this acts as regularization method by randomly ignoring 40% of nodes in each epoch (iteration).</li>
<li>the last output layer with 3 nodes since we have 3 class and with <strong>softmax</strong> function.</li>
</ol>
<p>In <strong>keras</strong> package the above steps will be coded as follows:</p>
<pre class="r"><code>model &lt;- keras_model_sequential()

model %&gt;% 
  layer_conv_2d(filters = 32,
                        kernel_size = c(3,3),
                        activation = &quot;relu&quot;,
                        input_shape = c(150,150,3))%&gt;%
  layer_max_pooling_2d(pool_size = c(3,3), strides = 2)%&gt;%
  layer_conv_2d(filters = 64,
               kernel_size = c(5,5),
                activation = &quot;relu&quot;) %&gt;%
  layer_max_pooling_2d(pool_size = c(2,2), strides = 2)%&gt;%
  layer_conv_2d(filters = 128,
                kernel_size = c(3,3),
                activation = &quot;relu&quot;) %&gt;%
  layer_max_pooling_2d(pool_size = c(2,2), strides = 2)%&gt;%
  layer_flatten()%&gt;%
  layer_dense(units=256)%&gt;% layer_activation_leaky_relu(alpha = 0.1)%&gt;%
  layer_dropout(rate=0.4)%&gt;%
  layer_dense(units=3, activation = &quot;softmax&quot;)</code></pre>
<p>We can figure out this architecture and how many parameters it has by calling the summary function.</p>
<pre class="r"><code>summary(model)</code></pre>
<pre><code>## Model: &quot;sequential&quot;
## ________________________________________________________________________________
## Layer (type)                        Output Shape                    Param #     
## ================================================================================
## conv2d (Conv2D)                     (None, 148, 148, 32)            896         
## ________________________________________________________________________________
## max_pooling2d (MaxPooling2D)        (None, 73, 73, 32)              0           
## ________________________________________________________________________________
## conv2d_1 (Conv2D)                   (None, 69, 69, 64)              51264       
## ________________________________________________________________________________
## max_pooling2d_1 (MaxPooling2D)      (None, 34, 34, 64)              0           
## ________________________________________________________________________________
## conv2d_2 (Conv2D)                   (None, 32, 32, 128)             73856       
## ________________________________________________________________________________
## max_pooling2d_2 (MaxPooling2D)      (None, 16, 16, 128)             0           
## ________________________________________________________________________________
## flatten (Flatten)                   (None, 32768)                   0           
## ________________________________________________________________________________
## dense (Dense)                       (None, 256)                     8388864     
## ________________________________________________________________________________
## leaky_re_lu (LeakyReLU)             (None, 256)                     0           
## ________________________________________________________________________________
## dropout (Dropout)                   (None, 256)                     0           
## ________________________________________________________________________________
## dense_1 (Dense)                     (None, 3)                       771         
## ================================================================================
## Total params: 8,515,651
## Trainable params: 8,515,651
## Non-trainable params: 0
## ________________________________________________________________________________</code></pre>
<p>As we see we have huge number of parameters <strong>8 515 651</strong>. Since the data has only 21 instances, the computation process in my laptop takes only few seconds. Howver, with large data set tihs model may take more time.</p>
<p>The last step before running the model is to specify the loss function, the optimizer and the metric.</p>
<ul>
<li><p>For multiclassification problem the most widely used one is <a href="https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23">categorical cross entropy</a>.</p></li>
<li><p>Besides the popular <strong>gradient descent</strong> <a href="https://keras.io/optimizers/">optimizer</a> (with its versions , <strong>stochastic gradient descent</strong> and <strong>mini batch gradient descent</strong>), there exist other ones such as <strong>adam</strong> , <strong>adadelta</strong>, <strong>mrsprop</strong> (the first one will be used for our case). In practice sometimes we finetune the hyperparameters by changing these optimizers.</p></li>
<li><p>For classification problems we have many <a href="https://keras.io/metrics/">metrics</a>, the famous ones are: <strong>accuracy</strong> (used for our case), <strong>roc</strong>, <strong>area under roc</strong>, <strong>precision</strong>.</p></li>
</ul>
<pre class="r"><code>model %&gt;% compile(loss= &quot;categorical_crossentropy&quot;,
                  optimizer=&quot;adam&quot;,
                  metrics=&quot;accuracy&quot;)</code></pre>
<p>At this stage everything is ready to train our model by calling the function <strong>fit</strong>. the epoch value is the number of iterations or the gradient descent steps,and the validation_split is the holdout samples used for assessment , here four images. I have run this model before and in oreder to avoide running it again i preceed the script by #.
if you want to run this model juste remove thid symbol.</p>
<pre class="r"><code>#history &lt;- model %&gt;%
  #fit(trainall, trainy, epoch=50, validation_split=0.2)</code></pre>
<p>unlike machine learning model in which we can set a seed to get the result reproducible, each time we rerun the model we get different result. In practice, we intentionally rerun the model to improve the model performance, and ones we get the best one we save it as follows:</p>
<pre class="r"><code>#save_model_hdf5(model, &quot;modelcnn.h5&quot;)</code></pre>
<p>And we can load it again as follows:</p>
<pre class="r"><code>model &lt;- load_model_hdf5(&quot;modelcnn.h5&quot;)</code></pre>
<p>The history object has all the necessary information such as the metric values for each epoch , so we can extract this informatiton to create a plot as follows.</p>
<pre class="r"><code>#train_loss &lt;- history$metrics$loss
#valid_loss &lt;- history$metrics$val_loss
#train_acc &lt;- history$metrics$accuracy
#valid_acc &lt;- history$metrics$val_accuracy
#epoch &lt;- 1:50</code></pre>
<pre class="r"><code>#df &lt;- tibble::tibble(epoch,train_loss,valid_loss,train_acc,valid_acc)</code></pre>
<pre class="r"><code>library(ggplot2)
#p1 &lt;- ggplot(df,aes(x=epoch, train_loss))+
 # geom_point(size=1, color=&quot;blue&quot;)+
#  geom_point(aes(x=epoch, valid_loss), size=1, color=&quot;red&quot;)+
 # ylab(&quot;Loss&quot;)
#ggsave(&quot;plot_loss.jpg&quot;, p1, device = &quot;jpeg&quot;, width = 20, height = 15, units = &quot;cm&quot;)</code></pre>
<p>AS you notice all the above codes have not been executed to avoide the issue discussed above and to make things simple. here we load the plot saved.</p>
<pre class="r"><code>par(mar=c(0,0,0,0))
plot(as.raster(readImage(&quot;plot_loss.jpg&quot;)))</code></pre>
<p><img src="/courses/cnnmodel/imageCNN_files/figure-html/unnamed-chunk-21-1.png" width="576" /></p>
<p>This plot shows the loss values for both the training set (in blue) and the validation set (in red), we see that the training loss consistently increases whereas the validation loss largely oscillating reflecting the less capability of the model to well predict the new unseen examples.
The same conclusion can be induced from the following plot for the acccuracy metric.</p>
<pre class="r"><code>#p2 &lt;- ggplot(df,aes(x=epoch, train_acc))+
  #geom_point(size=1, color=&quot;blue&quot;)+
  #geom_point(aes(x=epoch, valid_acc), size=1, color=&quot;red&quot;)+
 # ylab(&quot;accuracy&quot;)
#ggsave(&quot;plot_acc.jpg&quot;, p2, device = &quot;jpeg&quot;, width = 20, height = 15, units = &quot;cm&quot;)</code></pre>
<p>Here also we do the same thing</p>
<pre class="r"><code>par(mar=c(0,0,0,0))
plot(as.raster(readImage(&quot;plot_acc.jpg&quot;)))</code></pre>
<p><img src="/courses/cnnmodel/imageCNN_files/figure-html/unnamed-chunk-23-1.png" width="576" /></p>
<p><strong>Note</strong>: we coud have used directly the plot function , plot(history), but doing so we will get ifferent plot each time we knit the document.</p>
</div>
<div id="model-evaluation" class="section level2">
<h2>model evaluation</h2>
<p>We can evaluate the model performance using the training set as follows:</p>
<pre class="r"><code>train_evaluate&lt;- evaluate(model, trainall, trainy)</code></pre>
<p>With this first architecture we get a high accuracy rate <strong>95.24%</strong> and the loss is <strong>0.0832</strong>. However, becarefull with this rate sinc it is computed from the training data which in many cases reflects the <strong>overfitting</strong> problem<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. the best evaluation, thus, is that based on the testing set.</p>
<pre class="r"><code>test_evaluate&lt;- evaluate(model, testall, testy)</code></pre>
<p>Using the testing set that is not seen by the model, the accuracy rate is about 55.56%.
In fact this is exactly what we warned about it, indeed we have an overfiting problem where the model try to memorize every noisy pattern which will constrain the model to poorly generalize to unseen data.</p>
</div>
<div id="prediction" class="section level2">
<h2>Prediction</h2>
<p>We can get the predictions of the testing set as follows:</p>
<pre class="r"><code>pred &lt;- predict_classes(model,testall)
pred</code></pre>
<pre><code>## [1] 0 0 2 0 2 0 2 2 2</code></pre>
<p>the following picture shows which images from the testing set are correctly classified :</p>
<pre class="r"><code>pred[pred==0] &lt;- &quot;cat&quot;
pred[pred==1] &lt;- &quot;dog&quot;
pred[pred==2] &lt;- &quot;lion&quot;


par(mfrow=c(3,3))


foreach(i=1:9) %do% {display(test[[i]], method=&quot;raster&quot;);
  text(x = 20, y = 20, label = pred[i], 
       adj = c(0,1), col = &quot;black&quot;, cex = 4)
}</code></pre>
<p><img src="/courses/cnnmodel/imageCNN_files/figure-html/unnamed-chunk-27-1.png" width="576" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
<p>Using this model to predict the test examples, all the dogs are misclassified wherease the lions are perfectly classified.</p>
<p>We can also display the training examples as follows:</p>
<pre class="r"><code>pred1 &lt;- predict_classes(model,trainall)

pred1[pred1==0] &lt;- &quot;cat&quot;
pred1[pred1==1] &lt;- &quot;dog&quot;
pred1[pred1==2] &lt;- &quot;lion&quot;


par(mfrow=c(7,3))


foreach(i=1:21) %do% {display(train[[i]], method=&quot;raster&quot;);
  text(x = 20, y = 20, label = pred1[i], 
       adj = c(0,1), col = &quot;black&quot;, cex = 2)
}</code></pre>
<p><img src="/courses/cnnmodel/imageCNN_files/figure-html/unnamed-chunk-28-1.png" width="576" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>As we see this model perfectly identified cats and lions but failed to identify any of the dogs in the testing set which is not the case for the training data where the model has high accuracy, and as I mentioned earlier this the consequences of the overfiting problem. However, There are bunch of techniques that can used in such situation such as regularization methods (L2,L1), pooling, dropuot layers..ect. All these techniques will be adressed soon in further papers.
Besides overfiting, we can also improve the model by playing around with hyperparameters such as changing , the number of the layers, number of the nodes in each layer, number of epochs…ect.</p>
<p><strong>Note</strong>: Be aware that this model can not be reliable since it has used very small data. However, we may get a higher performance for this model if we implement very large dataset.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Francois chollet, Deep learning with R, Meap edition, 2017, P112 <a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>An introduction to statistical learning, Garth et al, spring, New York, page 33, <a href="ISBN:978-1-4614-7173-0" class="uri">ISBN:978-1-4614-7173-0</a><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
